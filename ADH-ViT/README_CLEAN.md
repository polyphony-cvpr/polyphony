# VideoMAEv2 - Clean Alternating Dual-Hand Training

This is a **clean, minimal version** of the VideoMAEv2 codebase containing only the essential files needed for **alternating dual-hand video action recognition training** on the HA-ViD dataset.

## ğŸ“ Directory Structure

```
VideoMAEv2_clean_alternating/
â”œâ”€â”€ run_alternating_hand_finetuning.py    # Main training script
â”œâ”€â”€ engine_for_alternating_finetuning.py  # Training engine
â”œâ”€â”€ engine_for_finetuning.py              # Helper functions (merge, etc.)
â”œâ”€â”€ utils.py                               # Utility functions
â”œâ”€â”€ optim_factory.py                       # Optimizer factory
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ modeling_finetune_alternating.py  # Alternating dual-head ViT model
â”‚   â””â”€â”€ vit_b_k710_dl_from_giant.pth     # Pretrained weights
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ build.py                          # Dataset builder
â”‚   â”œâ”€â”€ datasets.py                       # Dataset classes
â”‚   â”œâ”€â”€ functional.py                     # Data functions
â”‚   â”œâ”€â”€ loader.py                         # Data loader
â”‚   â”œâ”€â”€ masking_generator.py             # Masking utilities
â”‚   â”œâ”€â”€ rand_augment.py                   # Random augmentation
â”‚   â”œâ”€â”€ random_erasing.py                 # Random erasing
â”‚   â”œâ”€â”€ transforms.py                     # Transform utilities
â”‚   â”œâ”€â”€ video_transforms.py               # Video-specific transforms
â”‚   â””â”€â”€ volume_transforms.py              # Volume transforms
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ finetune/
â”‚       â””â”€â”€ train_havid_alternating.sh   # Training shell script
â”œâ”€â”€ requirements.txt                      # Python dependencies
â”œâ”€â”€ LICENSE                               # License file
â””â”€â”€ README.md                            # Original README

```

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
cd VideoMAEv2_clean_alternating
pip install -r requirements.txt
```

### 2. Prepare Data

Ensure your HA-ViD dataset is organized as:
```
data/havid_mmaction/
â”œâ”€â”€ lh_v0/
â”‚   â”œâ”€â”€ train_list_video.txt
â”‚   â”œâ”€â”€ val_list_video.txt
â”‚   â””â”€â”€ videos/
â””â”€â”€ rh_v0/
    â”œâ”€â”€ train_list_video.txt
    â”œâ”€â”€ val_list_video.txt
    â””â”€â”€ videos/
```

### 3. Run Training

```bash
bash scripts/finetune/train_havid_alternating.sh
```

Or run directly:

```bash
python run_alternating_hand_finetuning.py \
    --model vit_base_patch16_224_alternating \
    --lh_data_path /path/to/havid_mmaction/lh_v0 \
    --lh_data_root /path/to/havid_mmaction/lh_v0 \
    --rh_data_path /path/to/havid_mmaction/rh_v0 \
    --rh_data_root /path/to/havid_mmaction/rh_v0 \
    --lh_num_classes 75 \
    --rh_num_classes 75 \
    --data_set HAVID \
    --finetune models/vit_b_k710_dl_from_giant.pth \
    --output_dir output/havid_alternating_hands \
    --batch_size 4 \
    --epochs 50 \
    --alternation_steps 50
```

## ğŸ¯ Key Parameters

- `--alternation_steps`: Number of training steps before switching between left/right hand (default: 50)
- `--lh_num_classes` / `--rh_num_classes`: Number of action classes for each hand
- `--batch_size`: Batch size per GPU
- `--epochs`: Total training epochs
- `--lr`: Learning rate (default: 1e-3)
- `--drop_path`: DropPath rate (default: 0.3)

## ğŸ“Š Model Architecture

The model uses a **Vision Transformer (ViT) with dual classification heads**:
- **Shared backbone**: Extracts visual features from video frames
- **Left-hand head**: Classifies left-hand actions
- **Right-hand head**: Classifies right-hand actions
- **Alternating training**: Switches between hands every N steps

## ğŸ”§ What Was Removed

This clean version removes:
- âŒ Semantic feature alignment scripts (TCN-based)
- âŒ Dual-hand semantic integration
- âŒ Language conditioning modules
- âŒ Feature extraction scripts
- âŒ Evaluation and visualization scripts
- âŒ Multiple training strategy variants (v2, v3, v4)
- âŒ One-stream training variants
- âŒ Pretraining scripts
- âŒ Assembly101 and Breakfast dataset scripts
- âŒ Documentation files (except this README)
- âŒ Log files and checkpoints
- âŒ Experimental and debug scripts

**Total: ~70+ files removed, keeping only 23 essential files**

## ğŸ“ Files Breakdown

### Core Training (3 files)
- `run_alternating_hand_finetuning.py` - Main entry point
- `engine_for_alternating_finetuning.py` - Training/validation loops
- `engine_for_finetuning.py` - Helper functions

### Model (2 files)
- `models/__init__.py` - Model registry
- `models/modeling_finetune_alternating.py` - Alternating dual-head ViT

### Dataset (11 files)
- All files in `dataset/` directory - Data loading and augmentation

### Utils (2 files)
- `utils.py` - General utilities (distributed training, logging, etc.)
- `optim_factory.py` - Optimizer creation and layer-wise learning rate decay

### Config (3 files)
- `README.md` - Original project README
- `requirements.txt` - Python dependencies
- `LICENSE` - License information

### Scripts (1 file)
- `scripts/finetune/train_havid_alternating.sh` - Example training script

### Pretrained (1 file)
- `models/vit_b_k710_dl_from_giant.pth` - Pretrained ViT weights

## ğŸ”— Dependencies

Key dependencies (see `requirements.txt` for full list):
- PyTorch >= 1.8.0
- torchvision
- timm
- decord (for video loading)
- einops

## ğŸ“„ License

See LICENSE file for details.

## ğŸ™ Acknowledgments

This is a cleaned version of the VideoMAEv2 project, focusing only on alternating dual-hand training functionality for the HA-ViD dataset.

